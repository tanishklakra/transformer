# ðŸ§  GPT-Style Text Generation with PyTorch

---

## ðŸ“Œ Overview

This project is a **custom GPT-like character-level language model** implemented in **PyTorch**.  
It:

- Loads a vocabulary from `vocab.txt`
- Encodes and decodes text into token IDs
- Loads a pre-trained model (`model-1.pkl`)
- Generates text based on a user prompt using causal self-attention

The model architecture includes:

- Token & position embeddings
- Multi-head self-attention
- Feed-forward network
- Layer normalization
- Cross-entropy loss for training
- Sampling-based text generation

---

## âœ¨ Features

âœ… Custom GPT-like Transformer model  
âœ… Character-level encoding/decoding from vocabulary file  
âœ… Text generation with temperature-free multinomial sampling  
âœ… Uses **CUDA** if available for acceleration  
âœ… Can load from pre-trained `.pkl` checkpoint  

---

Main Components
Head â†’ Single self-attention head

MultiHeadAttention â†’ Multiple heads concatenated

FeedForward â†’ Fully connected layers for token processing

Block â†’ Transformer block with attention + feed-forward

GPTLanguageModel â†’ Stacks multiple blocks + embeddings + output head
